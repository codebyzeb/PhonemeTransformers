{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Probe for Word Boundaries\n",
    "\n",
    "Adding a linear probe to the pre-trained model to see if it can detect word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import yaml\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.model.model import next_char_transformer\n",
    "from src.segmentation.probing import WordBoundaryDataset, BoundaryProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_dir = 'wandb/run-20221110_041703-6926109/files' # old model, aligned\n",
    "run_dir = 'wandb/run-20221203_063741-9317928/files' # new model\n",
    "# run_dir = 'wandb/run-20221216_170649-829928499/files' # number model\n",
    "\n",
    "seed = 32\n",
    "\n",
    "checkpoint_path = run_dir + '/best.pt'\n",
    "with open(run_dir + '/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset...\n",
      "{'<PAD>': 0, '<BOUNDARY>': 1, 'dʒ': 2, 'ʌ': 3, 's': 4, 't': 5, 'l': 6, 'aɪ': 7, 'k': 8, 'j': 9, 'ʊɹ': 10, 'b': 11, 'ʊ': 12, 'æ': 13, 'h': 14, 'oʊ': 15, 'm': 16, 'd': 17, 'uː': 18, 'w': 19, 'ɑː': 20, 'n': 21, 'ə': 22, 'ð': 23, 'ɐ': 24, 'ɾ': 25, 'ɪ': 26, 'ɛ': 27, 'z': 28, 'iː': 29, 'ɛɹ': 30, 'f': 31, 'eɪ': 32, 'ɡ': 33, 'ᵻ': 34, 'p': 35, 'i': 36, 'əl': 37, 'tʃ': 38, 'θ': 39, 'ŋ': 40, 'oːɹ': 41, 'ɹ': 42, 'ɔɪ': 43, 'ɔː': 44, 'aʊ': 45, 'ɪɹ': 46, 'v': 47, 'ɜː': 48, 'ɚ': 49, 'ɑːɹ': 50, 'ɔːɹ': 51, 'ɔ': 52, 'ʃ': 53, 'æː': 54, 'aɪɚ': 55, 'iə': 56, 'ʔ': 57, 'n̩': 58, 'oː': 59, 'aɪə': 60, 'ʒ': 61, 'aɪʊɹ': 62, 'ɑ̃': 63, 'r': 64, 'ɫ': 65, 'ɬ': 66, 'aɪʊ': 67, 'ɛː': 68, 'ɐː': 69, 'nʲ': 70, 'x': 71, '(es)': 72, 'o': 73, 'a': 74, '(enus)': 75}\n"
     ]
    }
   ],
   "source": [
    "data_dir = config['root_path']['value']\n",
    "fn = 'corpus.{}.data'.format('.'.join(data_dir.split('/')))\n",
    "if os.path.exists(fn):\n",
    "    print('Loading cached dataset...')\n",
    "    corpus = torch.load(fn)\n",
    "    ntokens = len(corpus.dictionary)\n",
    "else:\n",
    "    print('No precached dataset found')\n",
    "    raise Exception('No precached dataset found')\n",
    "print (corpus.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data = WordBoundaryDataset('data/Eng-NA/valid.txt', corpus, max_sequence_length=config['sequence_length']['value'])\n",
    "test_data = WordBoundaryDataset('data/Eng-NA/test.txt', corpus, max_sequence_length=config['sequence_length']['value'])\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define New Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Loading on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint_path, 'rb') as f:\n",
    "    checkpoint = torch.load(f, map_location=device)\n",
    "model = next_char_transformer(ntokens,\n",
    "                                n_layers=config['n_layers']['value'],\n",
    "                                hidden_size=config['hidden_size']['value'],\n",
    "                                inner_linear=config['inner_linear']['value'],\n",
    "                                max_sequence_len=config['sequence_length']['value']).to(device)\n",
    "model.load_state_dict(checkpoint['learner_state_dict'], strict=False)\n",
    "model.eval()\n",
    "\n",
    "probe = BoundaryProbe(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9061,  0.6991],\n",
       "         [-0.6792,  1.1001],\n",
       "         [ 0.0355,  0.6220],\n",
       "         [-0.3786,  1.4314],\n",
       "         [-0.6435,  1.8600],\n",
       "         [-0.2370,  0.9272],\n",
       "         [-1.1037,  0.8358],\n",
       "         [-0.2219,  0.7312],\n",
       "         [-0.5623,  0.8842],\n",
       "         [-0.1591,  0.4556],\n",
       "         [-0.0991,  0.4577],\n",
       "         [-0.8836,  1.0321],\n",
       "         [-0.0318,  1.0325],\n",
       "         [-0.0408,  0.4338],\n",
       "         [-0.7807,  0.8874],\n",
       "         [-1.1760,  0.5431],\n",
       "         [-0.5551,  1.5818]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = train_dataloader.dataset[0]\n",
    "probe(input.unsqueeze(0).to(device), torch.ones(1, input.shape[0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.102730, Batch: 100/1000\n",
      "Epoch: 0, Loss: 0.303036, Batch: 200/1000\n",
      "Epoch: 0, Loss: 0.120319, Batch: 300/1000\n",
      "Epoch: 0, Loss: 0.054726, Batch: 400/1000\n",
      "Epoch: 0, Loss: 0.025972, Batch: 500/1000\n",
      "Epoch: 0, Loss: 0.056543, Batch: 600/1000\n",
      "Epoch: 0, Loss: 0.186891, Batch: 700/1000\n",
      "Epoch: 0, Loss: 0.123649, Batch: 800/1000\n",
      "Epoch: 0, Loss: 0.172437, Batch: 900/1000\n",
      "Epoch: 0, Loss: 0.059721, Batch: 1000/1000\n",
      "0: 0.954726\n",
      "OrderedDict([('token_precision', 0.7925716084356311), ('token_recall', 0.808346709470305), ('token_fscore', 0.8003814367450731), ('type_precision', 0.7696793002915452), ('type_recall', 0.6583541147132169), ('type_fscore', 0.7096774193548387), ('boundary_all_precision', 0.9365573378022504), ('boundary_all_recall', 0.9506682867557715), ('boundary_all_fscore', 0.9435600578871202), ('boundary_noedge_precision', 0.8782728525493799), ('boundary_noedge_recall', 0.9040189125295508), ('boundary_noedge_fscore', 0.8909599254426841)])\n",
      "Epoch: 1, Loss: 0.086875, Batch: 100/1000\n",
      "Epoch: 1, Loss: 0.042387, Batch: 200/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m mask \u001b[39m=\u001b[39m subsequent_mask(phonemes\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[39m=\u001b[39m probe(phonemes, mask)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(outputs[\u001b[39m0\u001b[39m], boundaries[\u001b[39m0\u001b[39m])\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/char_transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/segmentation/probing.py:63\u001b[0m, in \u001b[0;36mBoundaryProbe.forward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\" Take in and process masked src and target sequences. \"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m src_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membed(src)\n\u001b[0;32m---> 63\u001b[0m emb, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoder(src_emb, mask)\n\u001b[1;32m     64\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier_layer(emb)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/char_transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/model/model.py:51\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m intermediate_predictions \u001b[39m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 51\u001b[0m     x, prediction \u001b[39m=\u001b[39m layer(x, mask)\n\u001b[1;32m     52\u001b[0m     intermediate_predictions\u001b[39m.\u001b[39mappend(prediction)\n\u001b[1;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x), intermediate_predictions\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/char_transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/model/model.py:28\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_positional_encoding(x)\n\u001b[0;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msublayer[\u001b[39m0\u001b[39;49m](x, \u001b[39mlambda\u001b[39;49;00m x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x, mask))\n\u001b[1;32m     29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer[\u001b[39m1\u001b[39m](x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward)\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_prediction \u001b[39mor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_layer_predictions \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/char_transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/model/annotated_attention.py:72\u001b[0m, in \u001b[0;36mSublayerConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, sublayer):\n\u001b[1;32m     71\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mApply residual connection to any sublayer with the same size.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sublayer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x)))\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/model/model.py:28\u001b[0m, in \u001b[0;36mEncoderLayer.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_positional_encoding(x)\n\u001b[0;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer[\u001b[39m0\u001b[39m](x, \u001b[39mlambda\u001b[39;00m x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x, mask))\n\u001b[1;32m     29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer[\u001b[39m1\u001b[39m](x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward)\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_prediction \u001b[39mor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_layer_predictions \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/char_transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/model/annotated_attention.py:51\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m query, key, value \u001b[39m=\u001b[39m \\\n\u001b[1;32m     47\u001b[0m     [l(x)\u001b[39m.\u001b[39mview(nbatches, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     48\u001b[0m      \u001b[39mfor\u001b[39;00m l, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinears, (query, key, value))]\n\u001b[1;32m     50\u001b[0m \u001b[39m# 2) Apply attention on all the projected vectors in batch.\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m attention(query, key, value, mask\u001b[39m=\u001b[39;49mmask,\n\u001b[1;32m     52\u001b[0m                          dropout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout)\n\u001b[1;32m     54\u001b[0m \u001b[39m# 3) \"Concat\" using a view and apply a final linear.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous() \\\n\u001b[1;32m     56\u001b[0m     \u001b[39m.\u001b[39mview(nbatches, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k)\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/src/model/annotated_attention.py:22\u001b[0m, in \u001b[0;36mattention\u001b[0;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1e9\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m p_attn \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49msoftmax(scores, dim \u001b[39m=\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m dropout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     p_attn \u001b[39m=\u001b[39m dropout(p_attn)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/char_transformer/lib/python3.8/site-packages/torch/nn/functional.py:1834\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1833\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1834\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[1;32m   1835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1836\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "# BEST_MODEL_PATH = 'best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from src.data.data import subsequent_mask\n",
    "from src.segmentation.evaluate import evaluate\n",
    "\n",
    "optimizer = optim.SGD(probe.classifier_layer.parameters(), lr=0.001, momentum=0.9)\n",
    "length = len(train_dataloader)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    i = 0\n",
    "    for phonemes, boundaries in iter(train_dataloader):\n",
    "        phonemes = phonemes.to(device)\n",
    "        boundaries = boundaries.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = subsequent_mask(phonemes.shape[1])\n",
    "        outputs = probe(phonemes, mask)\n",
    "        loss = F.cross_entropy(outputs[0], boundaries[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: %d, Loss: %f, Batch: %d/%d' % (epoch, loss.item(), i, length))\n",
    "    \n",
    "    test_error_count = 0.0\n",
    "    total_boundaries = 0\n",
    "    gold_utterances = []\n",
    "    predicted_utterances = []\n",
    "    for phonemes, boundaries in iter(test_dataloader):\n",
    "        phonemes = phonemes.to(device)\n",
    "        boundaries = boundaries.to(device)\n",
    "        mask = subsequent_mask(phonemes.shape[1])\n",
    "        outputs = probe(phonemes, mask)\n",
    "        test_error_count += float(torch.sum(torch.abs(boundaries[0] - outputs[0].argmax(1))))\n",
    "        total_boundaries += outputs[0].shape[0]\n",
    "        predicted_boundaries = outputs[0].argmax(1)\n",
    "        gold_utterances.append(' '.join([(';eword ' if b.item() else '') + corpus.dictionary.idx2word[c.item()] for c, b in zip(phonemes[0,1:], boundaries[0,1:])]))\n",
    "        predicted_utterances.append(' '.join([(';eword ' if b.item() else '') + corpus.dictionary.idx2word[c.item()] for c, b in zip(phonemes[0,1:], predicted_boundaries[1:])]))\n",
    "    \n",
    "    results = evaluate(gold_utterances, predicted_utterances)\n",
    "    test_accuracy = 1.0 - float(test_error_count) / total_boundaries\n",
    "    print('%d: %f' % (epoch, test_accuracy))\n",
    "    print(results)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        # torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "char_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6eb6c929afcf603f51ff8ed9f7143d078a78b2b7ee276fa19494f172ecf2930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
