{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoible = pd.read_csv('../data/phoible.csv')\n",
    "phoible_phonemes = phoible.Phoneme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10\n",
    "\n",
    "def build_vocabulary(datasets, column='text'):\n",
    "\n",
    "    vocab = {'UNK' : 0, 'PAD' : 1, 'BOS' : 2, 'EOS' : 3, 'WORD_BOUNDARY' : 4, 'UTT_BOUNDARY' : 5}\n",
    "    unk_tokens = []\n",
    "    for dataset in datasets:\n",
    "        token_counts = {}\n",
    "        for line in dataset['text']:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token not in token_counts:\n",
    "                    token_counts[token] = 0\n",
    "                token_counts[token] += 1\n",
    "        \n",
    "        # Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "        for token, count in token_counts.items():\n",
    "            if count > MIN_COUNT and token not in vocab:\n",
    "                if token not in phoible_phonemes:\n",
    "                    unk_tokens.append(token)\n",
    "                else:\n",
    "                    vocab[token] = len(vocab)\n",
    "\n",
    "    print('Tokens not found in phoible: ', {token: token_counts[token] for token in unk_tokens})\n",
    "    print('Vocab: ', vocab)\n",
    "    return vocab\n",
    "\n",
    "def build_phoneme_tokenizer(vocab):\n",
    "\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.Replace(\"\\n\", \" UTT_BOUNDARY\"), normalizers.Strip()]\n",
    "    ) # Replace newlines with utterance boundaries\n",
    "    tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"BOS\", \"EOS\"])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "    wrapped_tokenizer.bos_token = \"BOS\"\n",
    "    wrapped_tokenizer.eos_token = \"EOS\"\n",
    "    wrapped_tokenizer.pad_token = \"PAD\"\n",
    "    wrapped_tokenizer.unk_token = \"UNK\"\n",
    "\n",
    "    return wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [load_dataset('transformersegmentation/CHILDES', 'English', split='train')]\n",
    "vocab = build_vocabulary(datasets)\n",
    "tokenizer = build_phoneme_tokenizer(vocab)\n",
    "tokenizer.push_to_hub('transformersegmentation/CHILDES-English-tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the BR Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/br/1.0.0/c1a2022b0fe6c73568543b5d30ee329425ac03b8b9f3d320d1fcc49917af66f1)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('transformersegmentation/CHILDES', 'br', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unknown token to UNK, and replace newlines with UTT_BOUNDARY\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"UNK\"))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"\\n\", \" UTT_BOUNDARY\"), normalizers.Strip()]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[\"UNK\", \"PAD\", \"BOS\", \"EOS\"], min_frequency=20)\n",
    "tokenizer.train_from_iterator(dataset['text'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.bos_token = \"BOS\"\n",
    "wrapped_tokenizer.eos_token = \"EOS\"\n",
    "wrapped_tokenizer.pad_token = \"PAD\"\n",
    "wrapped_tokenizer.unk_token = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BR-Tokenizer/commit/989424ca42e299a1583fc4681e55d46dc4a853e5', commit_message='Upload tokenizer', commit_description='', oid='989424ca42e299a1583fc4681e55d46dc4a853e5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub('transformersegmentation/BR-tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
