{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/english/1.0.0/c1a2022b0fe6c73568543b5d30ee329425ac03b8b9f3d320d1fcc49917af66f1)\n",
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/french/1.0.0/c1a2022b0fe6c73568543b5d30ee329425ac03b8b9f3d320d1fcc49917af66f1)\n"
     ]
    }
   ],
   "source": [
    "languages = ['english', 'french']\n",
    "train_datasets = [load_dataset('transformersegmentation/CHILDES', lang, split='train') for lang in languages]\n",
    "all_lines = []\n",
    "for dataset in train_datasets:\n",
    "    all_lines += dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'PAD': 1,\n",
       " 'BOS': 2,\n",
       " 'EOS': 3,\n",
       " 'WORD_BOUNDARY': 4,\n",
       " 'UTT_BOUNDARY': 5,\n",
       " 'dʒ': 6,\n",
       " 'ʌ': 7,\n",
       " 's': 8,\n",
       " 't': 9,\n",
       " 'l': 10,\n",
       " 'aɪ': 11,\n",
       " 'k': 12,\n",
       " 'j': 13,\n",
       " 'ʊɹ': 14,\n",
       " 'b': 15,\n",
       " 'ʊ': 16,\n",
       " 'æ': 17,\n",
       " 'h': 18,\n",
       " 'oʊ': 19,\n",
       " 'm': 20,\n",
       " 'd': 21,\n",
       " 'uː': 22,\n",
       " 'w': 23,\n",
       " 'ɑː': 24,\n",
       " 'n': 25,\n",
       " 'ə': 26,\n",
       " 'ð': 27,\n",
       " 'ɐ': 28,\n",
       " 'ɾ': 29,\n",
       " 'ɪ': 30,\n",
       " 'ɛ': 31,\n",
       " 'z': 32,\n",
       " 'iː': 33,\n",
       " 'ɛɹ': 34,\n",
       " 'f': 35,\n",
       " 'eɪ': 36,\n",
       " 'ɡ': 37,\n",
       " 'ᵻ': 38,\n",
       " 'p': 39,\n",
       " 'i': 40,\n",
       " 'əl': 41,\n",
       " 'tʃ': 42,\n",
       " 'θ': 43,\n",
       " 'ŋ': 44,\n",
       " 'oːɹ': 45,\n",
       " 'ɹ': 46,\n",
       " 'ɔɪ': 47,\n",
       " 'ɔː': 48,\n",
       " 'aʊ': 49,\n",
       " 'ɪɹ': 50,\n",
       " 'v': 51,\n",
       " 'ɜː': 52,\n",
       " 'ɚ': 53,\n",
       " 'ɑːɹ': 54,\n",
       " 'ɔːɹ': 55,\n",
       " 'ɔ': 56,\n",
       " 'ʃ': 57,\n",
       " 'æː': 58,\n",
       " 'aɪɚ': 59,\n",
       " 'iə': 60,\n",
       " 'ʔ': 61,\n",
       " 'n̩': 62,\n",
       " 'oː': 63,\n",
       " 'aɪə': 64,\n",
       " 'ʒ': 65,\n",
       " 'aɪʊɹ': 66,\n",
       " 'r': 67,\n",
       " 'ɫ': 68,\n",
       " 'aɪʊ': 69,\n",
       " 'y': 70,\n",
       " 'ɛ̃': 71,\n",
       " 'a': 72,\n",
       " 'ʁ': 73,\n",
       " 'e': 74,\n",
       " 'ɔ̃': 75,\n",
       " 'ɑ̃': 76,\n",
       " 'u': 77,\n",
       " 'o': 78,\n",
       " 'ø': 79,\n",
       " 'œ̃': 80,\n",
       " 'œ': 81,\n",
       " 'ɛː': 82,\n",
       " 'yː': 83,\n",
       " 'aː': 84,\n",
       " 'əʊ': 85,\n",
       " 'ɲ': 86,\n",
       " 'œː': 87,\n",
       " 'jː': 88}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocab = {'UNK' : 0, 'PAD' : 1, 'BOS' : 2, 'EOS' : 3, 'WORD_BOUNDARY' : 4, 'UTT_BOUNDARY' : 5}\n",
    "BAD_TOKENS = ['(', ')', 'fr', 'en', '(en)', '(fr)']\n",
    "MIN_COUNT = 10\n",
    "for dataset in train_datasets:\n",
    "    token_counts = {}\n",
    "    for line in dataset['text']:\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            if token not in token_counts:\n",
    "                token_counts[token] = 0\n",
    "            token_counts[token] += 1\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab and token not in BAD_TOKENS:\n",
    "            vocab[token] = len(vocab)\n",
    "vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unknown token to UNK, and replace newlines with UTT_BOUNDARY\n",
    "# tokenizer = Tokenizer(models.WordLevel(unk_token=\"UNK\"))\n",
    "# tokenizer.normalizer = normalizers.Sequence(\n",
    "#     [normalizers.Replace(\"\\n\", \"UTT_BOUNDARY\"), normalizers.Strip()]\n",
    "# )\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "# trainer = trainers.WordLevelTrainer(special_tokens=[\"UNK\", \"PAD\", \"BOS\", \"EOS\"], min_frequency=20)\n",
    "# tokenizer.train_from_iterator(all_lines, trainer=trainer)\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"\\n\", \" UTT_BOUNDARY\"), normalizers.Strip()]\n",
    ")\n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"BOS\", \"EOS\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.bos_token = \"BOS\"\n",
    "wrapped_tokenizer.eos_token = \"EOS\"\n",
    "wrapped_tokenizer.pad_token = \"PAD\"\n",
    "wrapped_tokenizer.unk_token = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/CHILDES-tokenizer/commit/6a4e30117f0b4451678f90b2a9c8c5c67859dd14', commit_message='Upload tokenizer', commit_description='', oid='6a4e30117f0b4451678f90b2a9c8c5c67859dd14', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub('transformersegmentation/CHILDES-tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the BR Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/br/1.0.0/c1a2022b0fe6c73568543b5d30ee329425ac03b8b9f3d320d1fcc49917af66f1)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('transformersegmentation/CHILDES', 'br', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unknown token to UNK, and replace newlines with UTT_BOUNDARY\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"UNK\"))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"\\n\", \" UTT_BOUNDARY\"), normalizers.Strip()]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[\"UNK\", \"PAD\", \"BOS\", \"EOS\"], min_frequency=20)\n",
    "tokenizer.train_from_iterator(dataset['text'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.bos_token = \"BOS\"\n",
    "wrapped_tokenizer.eos_token = \"EOS\"\n",
    "wrapped_tokenizer.pad_token = \"PAD\"\n",
    "wrapped_tokenizer.unk_token = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BR-Tokenizer/commit/989424ca42e299a1583fc4681e55d46dc4a853e5', commit_message='Upload tokenizer', commit_description='', oid='989424ca42e299a1583fc4681e55d46dc4a853e5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub('transformersegmentation/BR-tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
