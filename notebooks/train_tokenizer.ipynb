{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/English/1.0.0/095e19727e2d33f7808ec4d5c95d086a19ab190ee0ae9ded0d0f7532fa5652c8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset childes/French to /Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/French/1.0.0/095e19727e2d33f7808ec4d5c95d086a19ab190ee0ae9ded0d0f7532fa5652c8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49f9743a9914cd0b74462a65bfa7706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3ab9771a944627ba12ea9fdcf0f1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.88M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691c829c98ee47418350eceb54d18216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/536k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e58c870a7645e7af41a7e1ce70a654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/556k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5d59282b264bd28ed9f8bbc2871c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31194f425c2649d2ad39227bee506c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd1fe149e2d4d75bb0452606c6f171b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f943a16506e4f4f8a1f45d2c8d62832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset childes downloaded and prepared to /Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/French/1.0.0/095e19727e2d33f7808ec4d5c95d086a19ab190ee0ae9ded0d0f7532fa5652c8. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/German/1.0.0/095e19727e2d33f7808ec4d5c95d086a19ab190ee0ae9ded0d0f7532fa5652c8)\n"
     ]
    }
   ],
   "source": [
    "languages = ['English', 'French', 'German']\n",
    "train_datasets = [load_dataset('transformersegmentation/CHILDES', lang, split='train') for lang in languages]\n",
    "all_lines = []\n",
    "for dataset in train_datasets:\n",
    "    all_lines += dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/6tzh0bsj2txd1cz18gpcms_c0000gn/T/ipykernel_87724/494124095.py:1: DtypeWarning: Columns (4,7,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phoible = pd.read_csv('../data/phoible.csv')\n"
     ]
    }
   ],
   "source": [
    "phoible = pd.read_csv('../data/phoible.csv')\n",
    "phonemes = phoible.Phoneme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens not found in phoible:  ['ᵻ', 'ɔø', 'ʊɐ']\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'BOS': 2, 'EOS': 3, 'WORD_BOUNDARY': 4, 'UTT_BOUNDARY': 5, 'd̠ʒ': 6, 'ʌ': 7, 's': 8, 't': 9, 'l': 10, 'aɪ': 11, 'k': 12, 'j': 13, 'ʊ': 14, 'ɹ': 15, 'b': 16, 'æ': 17, 'h': 18, 'oʊ': 19, 'm': 20, 'iː': 21, 'ð': 22, 'ɛ': 23, 'z': 24, 'ɐ': 25, 'f': 26, 'eɪ': 27, 'w': 28, 'ɪ': 29, 'ɡ': 30, 'ɑː': 31, 'p': 32, 'uː': 33, 'i': 34, 'ɾ': 35, 'ə': 36, 't̠ʃ': 37, 'd': 38, 'θ': 39, 'ŋ': 40, 'oː': 41, 'ɔɪ': 42, 'ɔː': 43, 'n': 44, 'aʊ': 45, 'v': 46, 'ɜː': 47, 'ɚ': 48, 'ɔ': 49, 'ʃ': 50, 'æː': 51, 'ʔ': 52, 'n̩': 53, 'ʒ': 54, 'r': 55, 'ɫ': 56, 'y': 57, 'ɛ̃': 58, 'a': 59, 'ʁ': 60, 'e': 61, 'ɔ̃': 62, 'ɑ̃': 63, 'u': 64, 'o': 65, 'ø': 66, 'œ̃': 67, 'œ': 68, 'ɛː': 69, 'yː': 70, 'aː': 71, 'ɲ': 72, 'œː': 73, 'əʊ': 74, 'ts': 75, 'eː': 76, 'ç': 77, 'x': 78, 'ɛɪ': 79, 'ɜ': 80, 'ɑ': 81, 'ʏ': 82, 'pf': 83, 'øː': 84}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary by language, so that when we add more lanugages, the IDs don't change\n",
    "vocab = {'UNK' : 0, 'PAD' : 1, 'BOS' : 2, 'EOS' : 3, 'WORD_BOUNDARY' : 4, 'UTT_BOUNDARY' : 5}\n",
    "unk_tokens = []\n",
    "MIN_COUNT = 10\n",
    "for dataset in train_datasets:\n",
    "    token_counts = {}\n",
    "    for line in dataset['text']:\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            if token not in token_counts:\n",
    "                token_counts[token] = 0\n",
    "            token_counts[token] += 1\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab:\n",
    "            if token not in phonemes:\n",
    "                unk_tokens.append(token)\n",
    "            else:\n",
    "                vocab[token] = len(vocab)\n",
    "print('Tokens not found in phoible: ', unk_tokens)\n",
    "print('Vocab: ', vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unknown token to UNK, and replace newlines with UTT_BOUNDARY\n",
    "# tokenizer = Tokenizer(models.WordLevel(unk_token=\"UNK\"))\n",
    "# tokenizer.normalizer = normalizers.Sequence(\n",
    "#     [normalizers.Replace(\"\\n\", \"UTT_BOUNDARY\"), normalizers.Strip()]\n",
    "# )\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "# trainer = trainers.WordLevelTrainer(special_tokens=[\"UNK\", \"PAD\", \"BOS\", \"EOS\"], min_frequency=20)\n",
    "# tokenizer.train_from_iterator(all_lines, trainer=trainer)\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"\\n\", \" UTT_BOUNDARY\"), normalizers.Strip()]\n",
    ")\n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"BOS\", \"EOS\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.bos_token = \"BOS\"\n",
    "wrapped_tokenizer.eos_token = \"EOS\"\n",
    "wrapped_tokenizer.pad_token = \"PAD\"\n",
    "wrapped_tokenizer.unk_token = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/CHILDES-tokenizer/commit/eb227a47a632a826509b8d6d2d1065b91affbca6', commit_message='Upload tokenizer', commit_description='', oid='eb227a47a632a826509b8d6d2d1065b91affbca6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub('transformersegmentation/CHILDES-tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the BR Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset childes (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes/br/1.0.0/c1a2022b0fe6c73568543b5d30ee329425ac03b8b9f3d320d1fcc49917af66f1)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('transformersegmentation/CHILDES', 'br', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unknown token to UNK, and replace newlines with UTT_BOUNDARY\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"UNK\"))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"\\n\", \" UTT_BOUNDARY\"), normalizers.Strip()]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[\"UNK\", \"PAD\", \"BOS\", \"EOS\"], min_frequency=20)\n",
    "tokenizer.train_from_iterator(dataset['text'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "wrapped_tokenizer.bos_token = \"BOS\"\n",
    "wrapped_tokenizer.eos_token = \"EOS\"\n",
    "wrapped_tokenizer.pad_token = \"PAD\"\n",
    "wrapped_tokenizer.unk_token = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BR-Tokenizer/commit/989424ca42e299a1583fc4681e55d46dc4a853e5', commit_message='Upload tokenizer', commit_description='', oid='989424ca42e299a1583fc4681e55d46dc4a853e5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub('transformersegmentation/BR-tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
