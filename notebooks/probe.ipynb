{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Probe for Word Boundaries\n",
    "\n",
    "Adding a linear probe to the pre-trained model to see if it can detect word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import Optional, Tuple, Union\n",
    "from transformers import AutoTokenizer, GPT2ForTokenClassification\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: childes_english_na/full\n",
      "Found cached dataset childes_english_na (/Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes_english_na/full/1.0.0/fae6154891a4bae471c04c7f324ef76b5da7f8c32981606c5ceec3d1373563cf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52b89ca53c949e2828b7e60d96586e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('transformersegmentation/CHILDES_EnglishNA')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Segmenter:\n",
    "    def __init__(self, model_path, tokenizer_path):\n",
    "        self.model = GPT2ForTokenClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "    def segment(self, text):\n",
    "        input_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
    "        output = self.model(input_ids)\n",
    "        logits = output.logits\n",
    "        predictions = np.argmax(logits.detach().numpy(), axis=2)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        return pd.DataFrame({'token': tokens, 'prediction': predictions[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Probe(GPT2ForTokenClassification):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        config.num_labels = 2\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Freeze the transformer layer\n",
    "        with torch.no_grad():\n",
    "            transformer_outputs = self.transformer(\n",
    "                input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + transformer_outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at transformersegmentation/GPT2-gpt2_lm_head_model-model were not used when initializing GPT2Probe: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Probe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Probe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2Probe were not initialized from the model checkpoint at transformersegmentation/GPT2-gpt2_lm_head_model-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Probe.from_pretrained('transformersegmentation/GPT2-gpt2_lm_head_model-model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('transformersegmentation/Space-Tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import DataPreprocessor\n",
    "from src.config import DataPreprocessingParams\n",
    "\n",
    "dp = DataPreprocessor(DataPreprocessingParams(max_input_length=64, join_utts=False), tokenizer, labels_are_word_boundaries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0451534a8c4a4cc5957d3518d775be9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/898356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes_english_na/full/1.0.0/fae6154891a4bae471c04c7f324ef76b5da7f8c32981606c5ceec3d1373563cf/cache-61e2693e224df5e6.arrow\n",
      "Loading cached processed dataset at /Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes_english_na/full/1.0.0/fae6154891a4bae471c04c7f324ef76b5da7f8c32981606c5ceec3d1373563cf/cache-fe3230628325abab.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabf504a48df4e0a9bc4b83eee66dd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/898356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes_english_na/full/1.0.0/fae6154891a4bae471c04c7f324ef76b5da7f8c32981606c5ceec3d1373563cf/cache-5d3c002a83769b83.arrow\n",
      "Loading cached processed dataset at /Users/zebulongoriely/.cache/huggingface/datasets/transformersegmentation___childes_english_na/full/1.0.0/fae6154891a4bae471c04c7f324ef76b5da7f8c32981606c5ceec3d1373563cf/cache-ac3151bdb6ab61e2.arrow\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = dataset.map(\n",
    "        dp,\n",
    "        batched=True,\n",
    "        # num_proc=64,\n",
    "        remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# Remove all items that are shorter than the minimum length\n",
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda x: len(x[\"input_ids\"]) == 64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(torch\u001b[39m.\u001b[39mtensor(processed_dataset[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39m]), labels\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(processed_dataset[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong))\n",
      "File \u001b[0;32m~/Documents/UniDocs/PHD/research/projects/CharTransformers/TransformerSegmentation/env/lib/python3.9/site-packages/datasets/dataset_dict.py:57\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, k) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, (\u001b[39mstr\u001b[39m, NamedSplit)) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(k)\n\u001b[1;32m     58\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m             split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     61\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "model(torch.tensor(processed_dataset['input_ids'][0]), labels=torch.tensor(processed_dataset['labels'][0], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,             # total # of training epochs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=processed_dataset,         # training dataset\n",
    "    eval_dataset=processed_dataset,          # evaluation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.data.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m subsequent_mask\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msegmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate\n\u001b[1;32m     10\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(probe\u001b[39m.\u001b[39mclassifier_layer\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.data.data'"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "# BEST_MODEL_PATH = 'best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from src.data.data import subsequent_mask\n",
    "from src.segmentation.evaluate import evaluate\n",
    "\n",
    "optimizer = optim.SGD(probe.classifier_layer.parameters(), lr=0.001, momentum=0.9)\n",
    "length = len(train_dataloader)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    i = 0\n",
    "    for phonemes, boundaries in iter(train_dataloader):\n",
    "        phonemes = phonemes.to(device)\n",
    "        boundaries = boundaries.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = subsequent_mask(phonemes.shape[1])\n",
    "        outputs = probe(phonemes, mask)\n",
    "        loss = F.cross_entropy(outputs[0], boundaries[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: %d, Loss: %f, Batch: %d/%d' % (epoch, loss.item(), i, length))\n",
    "    \n",
    "    test_error_count = 0.0\n",
    "    total_boundaries = 0\n",
    "    gold_utterances = []\n",
    "    predicted_utterances = []\n",
    "    for phonemes, boundaries in iter(test_dataloader):\n",
    "        phonemes = phonemes.to(device)\n",
    "        boundaries = boundaries.to(device)\n",
    "        mask = subsequent_mask(phonemes.shape[1])\n",
    "        outputs = probe(phonemes, mask)\n",
    "        test_error_count += float(torch.sum(torch.abs(boundaries[0] - outputs[0].argmax(1))))\n",
    "        total_boundaries += outputs[0].shape[0]\n",
    "        predicted_boundaries = outputs[0].argmax(1)\n",
    "        gold_utterances.append(' '.join([(';eword ' if b.item() else '') + corpus.dictionary.idx2word[c.item()] for c, b in zip(phonemes[0,1:], boundaries[0,1:])]))\n",
    "        predicted_utterances.append(' '.join([(';eword ' if b.item() else '') + corpus.dictionary.idx2word[c.item()] for c, b in zip(phonemes[0,1:], predicted_boundaries[1:])]))\n",
    "    \n",
    "    results = evaluate(gold_utterances, predicted_utterances)\n",
    "    test_accuracy = 1.0 - float(test_error_count) / total_boundaries\n",
    "    print('%d: %f' % (epoch, test_accuracy))\n",
    "    print(results)\n",
    "    if test_accuracy > best_accuracy:\n",
    "        # torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "char_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6eb6c929afcf603f51ff8ed9f7143d078a78b2b7ee276fa19494f172ecf2930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
