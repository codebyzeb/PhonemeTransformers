[EXPERIMENT]
#run the transformer on the text8 corpus, as done in Al-Rfou et al (2016)
name=experiment_text8
#seed=32
use_wandb=True

[LOGGING]
log_interval=1000

# whether to save the final model (after training)
save_final_model=True

# whether to write checkpoints
save_checkpoints=True

# whether to save best model
save_best_model=True

###### TRAINING DATASET CONFIGS ######
[DATASET]
root_path=data/text8

###### MODEL ARCHITERTURE AND LEARNING ######

[MODEL]
hidden_size=512
n_layers=12
dropout=0.2
tied=False


[TRAINING]
lr=0.003
momentum=0.99
clip=0.25
epochs=100
batch_size=16
sequence_length=512
# can override the default device by specifying: 
# device=("cuda" or "cpu")
