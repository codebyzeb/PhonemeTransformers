[EXPERIMENT]
#run the transformer on the text8 corpus, as done in Al-Rfou et al (2016)
name=experiment_text8
#seed=32


###### TRAINING DATASET CONFIGS ######
[DATASET]
root_path=data/text8

###### MODEL ARCHITERTURE AND LEARNING ######

[MODEL]
hidden_size=512
n_layers=12
dropout=0.2
tied=False


[TRAINING]
lr=0.003
momentum=0.99
clip=0.25
epochs=40
batch_size=16
sequence_length=512
# can override the default device by specifying: 
# device=("cuda" or "cpu")

[LOGGING]
log_interval=1000

