name: 'gpt2_lm'

model_kwargs: {
  "n_layer": 6, 
  "n_head": 8,
  "n_embd": 256, 
  "n_positions": 256,
  "n_inner": 1024,
}
