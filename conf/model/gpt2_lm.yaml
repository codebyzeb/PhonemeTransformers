name: 'gpt2_lm_head_model'

resume_checkpoint_path: null

# kwargs for model initialization 
n_layer: 12 # Number of layers. Default for GPT-2 is 12
n_head: 12 # Number of attention heads. Default for GPT-2 is 12
n_embd : 128 # Size of embeddings. Default for GPT-2 is 768
n_positions: 256 # Maximum sequence length. Default for GPT-2 is 1024
n_inner: 1024Â  # Default for GPT-2 is 3072 (set to 4x n_embd)
