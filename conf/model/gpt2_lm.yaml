name: 'gpt2_lm_head_model'

resume_checkpoint_path: null

# kwargs for model initialization 
num_hidden_layers: 12
num_attention_heads: 8
hidden_size: 128
intermediate_size: 1024
initializer_range: 0.02  # stdev of trunc normal for initializing all weights
layer_norm_eps: 1e-5  # 1e-5 default in fairseq (and slightly better performance), 1e-12 default in hgugingface, 
dropout: 0.1