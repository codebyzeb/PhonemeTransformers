name: 'gpt2_lm'

model_kwargs: {
  "n_layer": 8, 
  "n_head": 8,
  "n_embd": 512, 
  "n_positions": 256,
  "n_inner": 2048,
}
