name: 'gpt2_lm'

model_kwargs: {
  "n_layer": 4, 
  "n_head": 4,
  "n_embd": 128, 
  "n_positions": 256,
  "n_inner": 512,
}
