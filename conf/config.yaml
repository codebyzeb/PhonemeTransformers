defaults: 
  - base_config
  - _self_
  - model: gpt2_800k

dataset: 
  name: 'transformersegmentation/CHILDES'
  subconfig: 'English'
  max_age: 120

experiment: 
  seed: 42 
  evaluate_segmentation: True
  evaluate_babyslm: False

data_preprocessing:
  max_input_length: 64
  join_utts: "dynamic"

tokenizer:
  name: 'transformersegmentation/CHILDES-Tokenizer' 

trainer: 
  batch_size: 32 # across 8 GPUs gives an effective batch size of 1024
  lr: 1e-3 # 1e-4 is used in fairseq; 1e-3 is default in huggingface
  num_warmup_steps: 60_000
  max_training_steps: 200_000
